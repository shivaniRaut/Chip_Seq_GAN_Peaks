{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import threading\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    loaded_data = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line by tabs\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 10 and parts[0][3:]!='X' and parts[0][3:]!='Y':\n",
    "                # Extract the required data\n",
    "                #print(\"**\",parts[0][3:],int(parts[0][3:])*1_000_000_000)\n",
    "                chr_value = int(parts[0][3:])*1_000_000_000\n",
    "                start_pos = int(parts[1])\n",
    "                end_pos = int(parts[2])\n",
    "                feature1 = float(parts[6])\n",
    "                feature2 = float(parts[7])\n",
    "                feature3 = float(parts[8])\n",
    "                feature4 = int(parts[9])\n",
    "\n",
    "                # Create a data point as a tuple\n",
    "                data_point = (chr_value, start_pos, end_pos, feature1, feature2, feature3, feature4)\n",
    "                loaded_data.append(data_point)\n",
    "    return loaded_data\n",
    "            \n",
    "def save_as_tfrecords_multithreaded(path, original_data, columns=[\"sequence\"], group_by_col=\"Label\"):\n",
    "    \"\"\"Provided data gets splitted in to groups and processed concurrently.\n",
    "    The outcome of this is a file per group.\n",
    "\n",
    "    Args:\n",
    "      path: Location where files should be stored\n",
    "      original_data: dataframe which should be converted into files\n",
    "      columns: a  list of columns which should be stored as sequences (Default value = [\"sequence\"])\n",
    "      group_by_col: a column name by which split data into groups (Default value = \"Label\")\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    threading_start = time.time()\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = []\n",
    "    data = original_data.groupby(group_by_col)\n",
    "    for group_id in data.groups:\n",
    "        if isinstance(group_id, str):\n",
    "            group_name = group_id.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "        elif isinstance(group_id, int):\n",
    "            group_name = str(group_id)\n",
    "        else:\n",
    "            group_name = \"_\".join([str(e) for e in group_id])\n",
    "        filename = os.path.join(path, group_name)\n",
    "        args = (filename, data.get_group(group_id), columns)\n",
    "        t = threading.Thread(target=save_as_tfrecords, args=args)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    coord.join(threads)\n",
    "    print(\"Completed all threads in {} seconds\".format(time.time() - threading_start))\n",
    "\n",
    "def save_as_tfrecords(filename, data, columns=[\"sequence\"], extension=\"tfrecords\"):\n",
    "    \"\"\"Processes a dataframe and stores data into tfrecord file\n",
    "\n",
    "    Args:\n",
    "      filename: the absolute path of the tfrecords file where data should be stored\n",
    "      data: dataframe containing data will be converted into tfrecord\n",
    "      columns: list of columns that should be stored as varying-length sequences (Default value = [\"sequence\"])\n",
    "      extension: file extension\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filename = \"{}.{}\".format(filename, extension)\n",
    "        with tf.io.TFRecordWriter(filename) as writer:\n",
    "            for index, row in data.iterrows():\n",
    "                feature = {\n",
    "                    'label': to_int_feature([row[0]])\n",
    "                }\n",
    "                for col_name in columns:\n",
    "                    value = row[col_name]\n",
    "                    if isinstance(value, int):\n",
    "                        feature[col_name] = to_int_feature([value])\n",
    "                    elif isinstance(value, float):\n",
    "                        feature[col_name] = to_float_feature([value])\n",
    "                    elif not isinstance(value, (list,)) and not isinstance (value, int) and ((value.dtype == np.float32) or (value.dtype == np.float64)):\n",
    "                        feature[col_name] = to_float_feature(value)\n",
    "                    else:\n",
    "                        feature[col_name] = to_int_feature(value)\n",
    "                        feature['length_' + col_name]:  to_int_feature([len(value)])\n",
    "\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                writer.write(example.SerializeToString())\n",
    "\n",
    "        print(\"Data was stored in {}\".format(filename))\n",
    "    except Exception as e:\n",
    "        print(\"Something went wrong went writting in to tfrecords file\")\n",
    "        print(\"Error is \", str(e))\n",
    "\n",
    "def to_int_feature(data):\n",
    "    \"\"\"\n",
    "    Converts int list to tf Feature\n",
    "    Args:\n",
    "        data: int list to be stored in tf record\n",
    "\n",
    "    Returns:\n",
    "        tf Feature that is used in building tfrecord\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=data))\n",
    "\n",
    "def to_float_feature(data):\n",
    "    \"\"\"\n",
    "    Converts float list to tf Feature\n",
    "    Args:\n",
    "        data: float list to be stored in tf record\n",
    "\n",
    "    Returns:\n",
    "        tf Feature that is used in building tfrecord\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was stored in /workspaces/Chip_Seq_GAN_Peaks/data/rep1_bed.tfrecords\n",
      "Completed all threads in 5.011658430099487 seconds\n",
      "Data was stored in /workspaces/Chip_Seq_GAN_Peaks/data/rep2_bed.tfrecords\n",
      "Completed all threads in 4.007375240325928 seconds\n",
      "Data was stored in /workspaces/Chip_Seq_GAN_Peaks/data/rep3_bed.tfrecords\n",
      "Completed all threads in 4.009684085845947 seconds\n",
      "Data was stored in /workspaces/Chip_Seq_GAN_Peaks/data/rep4_bed.tfrecords\n",
      "Completed all threads in 4.008080720901489 seconds\n",
      "Data was stored in /workspaces/Chip_Seq_GAN_Peaks/data/rep5_bed.tfrecords\n",
      "Completed all threads in 6.013237714767456 seconds\n",
      "Data was stored in /workspaces/Chip_Seq_GAN_Peaks/data/rep6_bed.tfrecords\n",
      "Completed all threads in 5.011481761932373 seconds\n"
     ]
    }
   ],
   "source": [
    "file_names = [\"rep1.bed\",\"rep2.bed\",\"rep3.bed\",\"rep4.bed\",\"rep5.bed\",\"rep6.bed\"]\n",
    "df_list = []\n",
    "for file_name in file_names:\n",
    "    loaded_data = load_data(file_name)\n",
    "    df = pd.DataFrame(loaded_data, columns=['chromosome','start','end','feature1','feature2','feature3','feature4'])\n",
    "    df[\"replica_id\"] = file_name\n",
    "    df_list.append(df)\n",
    "\n",
    "for df in df_list:\n",
    "    save_as_tfrecords_multithreaded(\"/workspaces/Chip_Seq_GAN_Peaks/data\", \n",
    "    df,['chromosome', 'start', 'end', 'feature1', 'feature2', 'feature3', 'feature4'],\"replica_id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
