{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tfrecords file\n",
    "#dataset = tf.data.TFRecordDataset(\"/workspaces/Chip_Seq_GAN_Peaks/data/fake/rep1_bed.tfrecords\")\n",
    "\n",
    "# Define a function to parse the TFRecords\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "#        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'chromosome': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'start': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'end': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'feature1': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'feature2': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'feature3': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'feature4': tf.io.FixedLenFeature([], tf.int64)\n",
    "#        ,'replica_id': tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    return example\n",
    "\n",
    "# Define a function to create a dataset from TFRecords files\n",
    "def create_dataset_from_tfrecords(tfrecord_pattern, batch_size=32, buffer_size=10000):\n",
    "    files = tf.data.Dataset.list_files(tfrecord_pattern)\n",
    "    dataset = files.interleave(\n",
    "        lambda filename: tf.data.TFRecordDataset(filename),\n",
    "        cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Specify the TFRecords pattern\n",
    "tfrecord_pattern_real = \"/workspaces/Chip_Seq_GAN_Peaks/data/real/*.tfrecords\"\n",
    "tfrecord_pattern_fake = \"/workspaces/Chip_Seq_GAN_Peaks/data/fake/*.tfrecords\"\n",
    "\n",
    "# Create a dataset\n",
    "batch_size = 32  # Adjust as needed\n",
    "buffer_size = 1500  # Adjust as needed\n",
    "dataset_real = create_dataset_from_tfrecords(tfrecord_pattern_real, batch_size=batch_size, buffer_size=buffer_size)\n",
    "dataset_fake = create_dataset_from_tfrecords(tfrecord_pattern_fake, batch_size=batch_size, buffer_size=buffer_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_41034/3605413461.py\", line 64, in train_step  *\n        real_output = discriminator(real_data, training=True)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 197, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Missing data for input \"dense_62_input\". You passed a data dictionary with keys ['chromosome', 'end', 'feature1', 'feature2', 'feature3', 'feature4', 'start']. Expected the following keys: ['dense_62_input']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/Chip_Seq_GAN_Peaks/model.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-space-robot-qvjj6p7qrpx29wxw/workspaces/Chip_Seq_GAN_Peaks/model.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-space-robot-qvjj6p7qrpx29wxw/workspaces/Chip_Seq_GAN_Peaks/model.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39mfor\u001b[39;00m real_data \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mbatch(batch_size):\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-space-robot-qvjj6p7qrpx29wxw/workspaces/Chip_Seq_GAN_Peaks/model.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m         train_step(real_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-space-robot-qvjj6p7qrpx29wxw/workspaces/Chip_Seq_GAN_Peaks/model.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsuper-duper-space-robot-qvjj6p7qrpx29wxw/workspaces/Chip_Seq_GAN_Peaks/model.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m, Generator Loss: \u001b[39m\u001b[39m{\u001b[39;00mgen_loss\u001b[39m}\u001b[39;00m\u001b[39m, Discriminator Loss: \u001b[39m\u001b[39m{\u001b[39;00mdisc_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileol4o0j1d.py:22\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(real_data)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m gen_tape, ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m disc_tape:\n\u001b[1;32m     21\u001b[0m     generated_data \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(generator), (ag__\u001b[39m.\u001b[39mld(noise),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)\n\u001b[0;32m---> 22\u001b[0m     real_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(discriminator), (ag__\u001b[39m.\u001b[39;49mld(real_data),), \u001b[39mdict\u001b[39;49m(training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), fscope)\n\u001b[1;32m     23\u001b[0m     fake_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(discriminator), (ag__\u001b[39m.\u001b[39mld(generated_data),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)\n\u001b[1;32m     24\u001b[0m     gen_loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(generator_loss), (ag__\u001b[39m.\u001b[39mld(fake_output),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/src/engine/input_spec.py:197\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m names:\n\u001b[1;32m    196\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m inputs:\n\u001b[0;32m--> 197\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMissing data for input \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    199\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou passed a data dictionary with keys \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected the following keys: \u001b[39m\u001b[39m{\u001b[39;00mnames\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         )\n\u001b[1;32m    203\u001b[0m     list_inputs\u001b[39m.\u001b[39mappend(inputs[name])\n\u001b[1;32m    204\u001b[0m inputs \u001b[39m=\u001b[39m list_inputs\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_41034/3605413461.py\", line 64, in train_step  *\n        real_output = discriminator(real_data, training=True)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 197, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Missing data for input \"dense_62_input\". You passed a data dictionary with keys ['chromosome', 'end', 'feature1', 'feature2', 'feature3', 'feature4', 'start']. Expected the following keys: ['dense_62_input']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to build the generator model\n",
    "def build_generator():\n",
    "    # Define your generator model architecture\n",
    "    # Example:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, input_shape=(100,), activation='relu'))\n",
    "    model.add(layers.Dense(7, activation='linear'))\n",
    "    return model\n",
    "\n",
    "# Define a function to build the discriminator model\n",
    "def build_discriminator():\n",
    "    # Define your discriminator model architecture\n",
    "    # Example:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, input_shape=(7,), activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Define the loss functions\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "# Define optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# Define a function to generate synthetic data\n",
    "def generate_synthetic_data(batch_size=32):\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "    generated_data = generator(noise, training=False)\n",
    "    return generated_data\n",
    "\n",
    "# Create the generator and discriminator\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Training loop\n",
    "@tf.function\n",
    "def train_step(real_data):\n",
    "    \"\"\"    real_data = tf.concat([\n",
    "            tf.cast(real_data[0], tf.float32),\n",
    "            tf.cast(real_data[1], tf.float32),\n",
    "            tf.cast(real_data[2], tf.float32),\n",
    "            tf.cast(real_data[3], tf.float32),\n",
    "            tf.cast(real_data[4], tf.float32),\n",
    "            tf.cast(real_data[5], tf.float32),\n",
    "            tf.cast(real_data[6], tf.float32)\n",
    "        ], axis=-1)\n",
    "    \"\"\"\n",
    "    batch_size = real_data['feature1'].shape[0]\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_data = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(generated_data, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "# Load your TFRecords dataset\n",
    "dataset = dataset_real\n",
    "\n",
    "# Training the GAN\n",
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for real_data in dataset.batch(batch_size):\n",
    "        train_step(real_data)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Generator Loss: {gen_loss}, Discriminator Loss: {disc_loss}\")\n",
    "\n",
    "# Generate synthetic data using the trained generator\n",
    "synthetic_data = generate_synthetic_data(batch_size=1000)\n",
    "print(\"Generated Synthetic Data:\")\n",
    "print(synthetic_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = real_data\n",
    "dataset['chromosome'] = tf.cast(dataset['chromosome'], tf.float32)\n",
    "dataset['start'] = tf.cast(dataset['start'], tf.float32)\n",
    "dataset['end'] = tf.cast(dataset['end'], tf.float32)\n",
    "dataset['feature1'] = tf.cast(dataset['feature1'], tf.float32)\n",
    "dataset['feature2'] = tf.cast(dataset['feature2'], tf.float32)\n",
    "dataset['feature3'] = tf.cast(dataset['feature3'], tf.float32)\n",
    "dataset['feature4'] = tf.cast(dataset['feature4'], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = tf.concat(\n",
    "    [dataset['chromosome'], dataset['start'], dataset['end'],\n",
    "     dataset['feature1'], dataset['feature2'], dataset['feature3'], dataset['feature4']], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 224), dtype=float32, numpy=\n",
       "array([[1.30e+10, 1.70e+10, 9.00e+09, ..., 1.01e+02, 1.46e+02, 1.93e+02],\n",
       "       [1.20e+10, 9.00e+09, 1.00e+09, ..., 7.80e+01, 9.70e+01, 1.71e+02],\n",
       "       [1.70e+10, 1.70e+10, 1.00e+09, ..., 9.80e+01, 1.29e+02, 2.16e+02],\n",
       "       ...,\n",
       "       [1.10e+10, 1.00e+09, 1.90e+10, ..., 2.77e+02, 2.49e+02, 1.42e+02],\n",
       "       [1.00e+09, 2.00e+09, 1.00e+09, ..., 2.53e+02, 1.68e+02, 1.68e+02],\n",
       "       [1.90e+10, 3.00e+09, 6.00e+09, ..., 1.44e+02, 1.33e+02, 2.10e+02]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
